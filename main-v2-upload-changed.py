# -*- coding: utf-8 -*-
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import FAISS, ElasticsearchStore
from elasticsearch import Elasticsearch
from config import *  # å¯¼å…¥ OLLAMA_HOST å’Œå…¶ä»–é…ç½®
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field
from typing import List, Dict, Any
import hashlib
import json
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import gradio as gr
import os
import socket
import webbrowser
from pdfminer.high_level import extract_text_to_fp
from io import StringIO
import tkinter as tk
from tkinter import filedialog
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from assistant_fun import *

requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

def load_pdf(pdfpath):
    documents = []
    for filename in os.listdir(pdfpath):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(pdfpath, filename)
            loader = PyPDFLoader(pdf_path)
            document = loader.load()
            documents.extend(document)
    return documents

def upload_files():
    try:
        root = tk.Tk()
        root.withdraw()
        root.attributes("-topmost", True)
        file_path = filedialog.askdirectory()
        root.destroy()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return []

    print('1.å¼€å§‹æ‰§è¡Œæ•°æ®é¢„å¤„ç†ä¸å­˜å‚¨æ­¥éª¤...')
    file_info = []

    # åˆå§‹åŒ– Elasticsearch è¿æ¥
    print('2.åˆå§‹åŒ– Elasticsearch å¹¶åˆ›å»ºç´¢å¼•...')
    es = Elasticsearch(hosts=[ES_HOST])
    index_settings = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "my_ik_analyzer": {
                        "type": "custom",
                        "tokenizer": "ik_max_word"
                    }
                }
            },
            # æ·»åŠ å‘é‡æ£€ç´¢æ”¯æŒï¼ˆå‡è®¾ä½¿ç”¨ dense_vector å­—æ®µï¼‰
            "number_of_shards": 1,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                "text": {
                    "type": "text",
                    "analyzer": "my_ik_analyzer",
                    "search_analyzer": "my_ik_analyzer"
                },
                "embedding": {  # æ·»åŠ å‘é‡å­—æ®µ
                    "type": "dense_vector",
                    "dims": 384  # æ ¹æ®åµŒå…¥æ¨¡å‹è°ƒæ•´ç»´åº¦ï¼Œä¾‹å¦‚ all-MiniLM-L6-v2 æ˜¯ 384
                }
            }
        }
    }

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œè‹¥å­˜åœ¨åˆ™åˆ é™¤å¹¶é‡æ–°åˆ›å»º
    try:
        if es.indices.exists(index=ES_INDEX):
            print(f"ç´¢å¼• '{ES_INDEX}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
            es.indices.delete(index=ES_INDEX)
        es.indices.create(index=ES_INDEX, body=index_settings)
        print(f"æˆåŠŸåˆ›å»ºç´¢å¼• '{ES_INDEX}'")
    except Exception as e:
        print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        return []

    print('3.å¤„ç†docxæ–‡ä»¶...', file_path)
    loader = DirectoryLoader(file_path, glob="**/*.docx")
    documents1 = loader.load()
    print(f'4.æˆåŠŸåŠ è½½ {len(documents1)} ä¸ªæ–‡æ¡£ã€‚')
    file_info1 = get_docx_info(file_path, 'docx')
    print('å¤„ç†pdfæ–‡ä»¶')
    documents2 = load_pdf(file_path)
    file_info2 = get_docx_info(file_path, 'pdf')

    file_info = file_info1 + file_info2
    documents = documents1 + documents2

    print(file_info)
    print('5.æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†å—...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n# ", "\n## ", "\n### ", "\n\n"]
    )
    chunks = text_splitter.split_documents(documents)
    print(f'6.åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚')

    print('7.æ­£åœ¨åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...')
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )

    print('8.æ­£åœ¨å°†æ–‡æœ¬å—å­˜å‚¨åˆ°FAISS...')
    faiss_chunks = [
        Document(
            page_content=chunk.page_content,
            metadata={**chunk.metadata, "retriever_source": "faiss"}
        ) for chunk in chunks
    ]
    vectorstore = FAISS.from_documents(faiss_chunks, embeddings, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)
    vectorstore.save_local(str(FAISS_INDEX))
    print('9.FAISSå­˜å‚¨å®Œæˆã€‚')

    print("æ£€æŸ¥åˆ†å—å…ƒæ•°æ®ç¤ºä¾‹ï¼š")
    if len(chunks) > 0:
        sample_chunk = chunks[0]
        print(f"å†…å®¹é•¿åº¦: {len(sample_chunk.page_content)}")
        print(f"å…ƒæ•°æ®: {sample_chunk.metadata}")

    print('10.æ­£åœ¨å°†æ–‡æœ¬å—å­˜å…¥Elasticsearch...')
    es_chunks = [
        Document(
            page_content=chunk.page_content,
            metadata={**chunk.metadata, "retriever_source": "es"}
        ) for chunk in chunks
    ]
    try:
        ElasticsearchStore.from_documents(
            documents=es_chunks,
            embedding=embeddings,
            es_connection=es,
            index_name=ES_INDEX,
            strategy=ElasticsearchStore.ApproxRetrievalStrategy(
                hybrid=True,
                query_model_id="sentence-transformers/all-MiniLM-L6-v2"
            )
        )
        print('11.æˆåŠŸå­˜å…¥Elasticsearchã€‚')
    except Exception as e:
        print(f"å­˜å‚¨åˆ°Elasticsearchå¤±è´¥: {str(e)}")
        return []

    print('12.éªŒè¯Elasticsearchå­˜å‚¨...')
    try:
        test_query = {"query": {"match_all": {}}, "size": 1}
        test_result = es.search(index=ES_INDEX, body=test_query)
        if test_result['hits']['total']['value'] > 0:
            print("ESå­˜å‚¨éªŒè¯æˆåŠŸï¼Œé¦–æ¡è®°å½•å…ƒæ•°æ®:", test_result['hits']['hits'][0]['_source'].get('metadata'))
        else:
            print("ESå­˜å‚¨éªŒè¯å¤±è´¥ï¼")
    except Exception as e:
        print(f"éªŒè¯Elasticsearchå­˜å‚¨å¤±è´¥: {str(e)}")

    try:
        count_response = es.count(index=ES_INDEX)
        file_count = count_response['count']
        print(f"æ–‡ä»¶æ€»æ•°: {file_count}")
    except Exception as e:
        print(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {
        "query": {"match_all": {}},
        "_source": ["metadata.source"],
        "size": 10000
    }
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        fileList = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source:
                filename = os.path.basename(source)
                if filename in seen_filenames:
                    continue
                seen_filenames.add(filename)
                _, file_extension = os.path.splitext(filename)
                file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
                try:
                    file_size = os.path.getsize(source) / (1024 * 1024)
                    file_size = round(file_size, 2)
                except FileNotFoundError:
                    file_size = 'æœªçŸ¥'
                print(f"æ–‡ä»¶å: {filename}, æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶å¤§å°: {file_size} MB")
                info = [str(filename), str(file_type), f"{file_size:.2f}"]
                fileList.append(info)
        return fileList
    except Exception as e:
        print(f"æœç´¢Elasticsearchå¤±è´¥: {str(e)}")
        return []

def Load_file_info_FrmES():
    es = Elasticsearch(hosts=[ES_HOST])
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    try:
        if not es.indices.exists(index=ES_INDEX):
            print(f"ç´¢å¼• '{ES_INDEX}' ä¸å­˜åœ¨ï¼Œæœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚")
            return []
    except Exception as e:
        print(f"æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§å¤±è´¥: {str(e)}")
        return []

    # è·å–æ–‡ä»¶æ€»æ•°
    try:
        count_response = es.count(index=ES_INDEX)
        file_count = count_response['count']
    except Exception as e:
        print(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {
        "query": {"match_all": {}},
        "_source": ["metadata.source"],
        "size": 10000
    }
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        fileList = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source:
                filename = os.path.basename(source)
                if filename in seen_filenames:
                    continue
                seen_filenames.add(filename)
                _, file_extension = os.path.splitext(filename)
                file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
                try:
                    file_size = os.path.getsize(source) / (1024 * 1024)
                    file_size = round(file_size, 2)
                except FileNotFoundError:
                    file_size = 'æœªçŸ¥'
                print(f"æ–‡ä»¶å: {filename}, æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶å¤§å°: {file_size} MB")
                info = [str(filename), str(file_type), f"{file_size:.2f}"]
                fileList.append(info)
        return fileList
    except Exception as e:
        print(f"æœç´¢Elasticsearchå¤±è´¥: {str(e)}")
        return []

def rag_chain(question):
    print('13.æ­£åœ¨åŠ è½½æ£€ç´¢å™¨...')
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_db = FAISS.load_local(str(FAISS_INDEX), embeddings, allow_dangerous_deserialization=True)
    es = Elasticsearch(hosts=[ES_HOST])

    class CustomESRetriever(BaseRetriever, BaseModel):
        es: Elasticsearch = Field(...)
        es_index: str = Field(...)
        k: int = Field(default=15)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                body = {
                    "query": {
                        "match": {
                            "text": {"query": query, "analyzer": "my_ik_analyzer"}
                        }
                    },
                    "size": self.k
                }
                response = self.es.search(index=self.es_index, body=body)
                return [
                    Document(
                        page_content=hit["_source"]["text"],
                        metadata=hit["_source"].get("metadata", {})
                    ) for hit in response['hits']['hits']
                ]
            except Exception as e:
                print(f"Elasticsearchæ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    es_retriever = CustomESRetriever(es=es, es_index=ES_INDEX)

    class CustomFAISSRetriever(BaseRetriever, BaseModel):
        vectorstore: FAISS = Field(...)
        score_threshold: float = Field(default=0.5)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=20)
                return [doc for doc, score in docs_with_scores if score >= self.score_threshold]
            except Exception as e:
                print(f"FAISSæ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    faiss_retriever = CustomFAISSRetriever(vectorstore=vector_db, score_threshold=0.5)

    print('å¹¶è¡Œæ£€ç´¢')
    es_docs, es_scores = retrieve_with_scores(es_retriever, question)
    faiss_docs, faiss_scores = retrieve_with_scores(faiss_retriever, question)

    print('åˆå¹¶æŸ¥è¯¢ç»“æœ')
    merged_results = merge_results(
        es_docs=es_docs,
        faiss_docs=faiss_docs,
        es_scores=es_scores,
        faiss_scores=faiss_scores,
        es_weight=0.4,
        faiss_weight=0.6,
        merge_strategy="reciprocal_rank"
    )
    print(merged_results)

    final_docs = [
        validate_metadata(Document(page_content=res.content, metadata=res.metadata))
        for res in merged_results
    ]

    print('åˆå¹¶åç»“æœ:')
    print(final_docs)

    formatted_docs = format_documents(final_docs[:3])
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆåŒã€æ ‡ä¹¦æ–‡æ¡£ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ï¼š
                {formatted_docs}

                é—®é¢˜ï¼š{question}
                """
    print('æ¨¡å‹å¼€å§‹é—®ç­” prompt:', prompt)
    try:
        response = session.post(
            f"{OLLAMA_HOST}/api/generate",
            json={
                "model": "deepseek-r1:14b",
                "prompt": prompt,
                "stream": True
            },
            timeout=120,
            stream=True
        )
        full_answer = ""
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                full_answer += chunk
                yield full_answer
    except Exception as e:
        print(f"æ¨¡å‹ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
        yield "ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"

# Gradio UI å’Œå…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œä»…æ›´æ–°æ ¸å¿ƒå‡½æ•°
with gr.Blocks(
    title="hi,GBEä»Šå¤©æ€ä¹ˆæ ·",
    css="""
        .gradio-container {max-width: 2000px !important}
        .answer-box {min-height: 500px !important;}
        .left-panel {padding-right: 20px; border-right: 1px solid #eee;}
        .right-panel {height: 100vh;}
        .wide-row { width: 80%; }
        .green-button {background-color: green; color: white;}
        .blue-button {background-color: #ADD8E6; color: white;}
        .gradio-label {font-size: 8px !important; font-weight: normal !important;}
        .gradio-container input {font-size: 8px !important;}
        .gradio-container textbox {font-size: 8px !important;}
        .gray-background textarea, .gray-background input[type="text"] {background-color: #cccccc !important;}
        .large-font {font-size: 72px !important;}
        .bold-font {font-weight: bold !important;}
        .center-text {text-align: center !important;}
        .red-text {color: red !important;}
    """
) as demo:
    gr.Markdown("## RAG.(ES+FAISS+Deepseek-R1:14b).", elem_classes="blue-button large-font font-weight center-text")

    with gr.Row():
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ DOCX/PDFæ–‡ä»¶è·¯å¾„")
            with gr.Group():
                upload_btn = gr.Button("æ–‡ä»¶å­˜å‚¨åˆ°ES/FAISSåº“", variant="primary")
                upload_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)

            gr.Markdown("## â“ æé—®")
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=4,
                    placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    elem_id="question-input"
                )
                ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", elem_classes="green-button")

        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## å·²ç»å­˜å‚¨æ–‡ä»¶åˆ—è¡¨ ")
            file_df = gr.Dataframe(
                headers=["æ–‡ä»¶å", "æ–‡ä»¶ç±»å‹", "å¤§å°(MB)"],
                datatype=["str", "str", "str"],
                interactive=False,
                elem_classes="File_list-box",
                show_copy_button=True
            )

            gr.Markdown("## ğŸ“ ç­”æ¡ˆ", elem_classes="blue-button")
            answer_output = gr.Textbox(
                label="å›ç­”",
                interactive=False,
                lines=25,
                elem_classes="answer-box",
                autoscroll=True,
                show_copy_button=True
            )
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)

    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    ask_btn.click(fn=rag_chain, inputs=question_input, outputs=[answer_output], show_progress="hidden")
    upload_btn.click(fn=upload_files, inputs=None, outputs=file_df)
    demo.load(fn=lambda: Load_file_info_FrmES(), inputs=None, outputs=file_df)

demo._js = """
function gradioApp() {
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}
"""

def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0

def check_environment():
    try:
        model_check = session.post(
            f"{OLLAMA_HOST}/api/show",
            json={"name": "deepseek-r1:14b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False

        response = session.get(
            f"{OLLAMA_HOST}/api/tags",
            proxies={"http": None, "https": None},
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)

    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)

    try:
        ollama_check = session.get(OLLAMA_HOST, timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)

        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")