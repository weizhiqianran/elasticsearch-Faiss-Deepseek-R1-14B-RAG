# -*- coding: utf-8 -*-
import os
import time
import json
import hashlib
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from typing import List
from pydantic import BaseModel, Field

import streamlit as st
from elasticsearch import Elasticsearch
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_community.vectorstores import FAISS, ElasticsearchStore
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain.text_splitter import RecursiveCharacterTextSplitter

from config import *  # å¯¼å…¥ OLLAMA_HOST å’Œå…¶ä»–é…ç½®
from assistant_fun import *

# é…ç½® requests é‡è¯•æœºåˆ¶
requests.adapters.DEFAULT_RETRIES = 3
session = requests.Session()
retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
session.mount('http://', HTTPAdapter(max_retries=retries))


# --------------------
# æ–‡ä»¶åŠ è½½ä¸å¤„ç†å‡½æ•°
# --------------------
def load_pdf(pdfpath: str) -> List[Document]:
    """åŠ è½½æŒ‡å®šè·¯å¾„ä¸­çš„æ‰€æœ‰ PDF æ–‡ä»¶"""
    documents = []
    for filename in os.listdir(pdfpath):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(pdfpath, filename)
            loader = PyPDFLoader(pdf_path)
            document = loader.load()
            documents.extend(document)
    return documents


# --------------------
# æ•°æ®å­˜å‚¨ä¸ç´¢å¼•å‡½æ•°
# --------------------
def upload_files(file_path: str) -> List[List[str]]:
    """ä¸Šä¼ æ–‡ä»¶å¹¶å­˜å‚¨åˆ° Elasticsearch å’Œ FAISS"""
    st.write('1. å¼€å§‹æ‰§è¡Œæ•°æ®é¢„å¤„ç†ä¸å­˜å‚¨æ­¥éª¤...')
    file_info = []

    # åˆå§‹åŒ– Elasticsearch è¿æ¥å’Œç´¢å¼•
    st.write('2. åˆå§‹åŒ– Elasticsearch å¹¶åˆ›å»ºç´¢å¼•...')
    es = Elasticsearch(hosts=[ES_HOST])
    index_settings = {
        "settings": {
            "analysis": {"analyzer": {"my_ik_analyzer": {"type": "custom", "tokenizer": "ik_max_word"}}},
            "number_of_shards": 1,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                "text": {"type": "text", "analyzer": "my_ik_analyzer", "search_analyzer": "my_ik_analyzer"},
                "embedding": {"type": "dense_vector", "dims": 384}  # é€‚é… all-MiniLM-L6-v2 æ¨¡å‹
            }
        }
    }

    # æ£€æŸ¥å¹¶é‡å»ºç´¢å¼•
    try:
        if es.indices.exists(index=ES_INDEX):
            st.write(f"ç´¢å¼• '{ES_INDEX}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
            es.indices.delete(index=ES_INDEX)
        es.indices.create(index=ES_INDEX, body=index_settings)
        st.write(f"æˆåŠŸåˆ›å»ºç´¢å¼• '{ES_INDEX}'")
    except Exception as e:
        st.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        return []

    # åŠ è½½å¹¶å¤„ç†æ–‡ä»¶
    st.write('3. å¤„ç† DOCX æ–‡ä»¶...')
    loader = DirectoryLoader(file_path, glob="**/*.docx")
    documents1 = loader.load()
    st.write(f'4. æˆåŠŸåŠ è½½ {len(documents1)} ä¸ª DOCX æ–‡æ¡£ã€‚')
    file_info1 = get_docx_info(file_path, 'docx')

    st.write('5. å¤„ç† PDF æ–‡ä»¶...')
    documents2 = load_pdf(file_path)
    file_info2 = get_docx_info(file_path, 'pdf')

    file_info = file_info1 + file_info2
    documents = documents1 + documents2

    # æ™ºèƒ½åˆ†å—
    st.write('6. æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†å—...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=50, separators=["\n# ", "\n## ", "\n### ", "\n\n"]
    )
    chunks = text_splitter.split_documents(documents)
    st.write(f'7. åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚')

    # åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
    st.write('8. æ­£åœ¨åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...')
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': True}
    )

    # å­˜å‚¨åˆ° FAISS
    st.write('9. æ­£åœ¨å°†æ–‡æœ¬å—å­˜å‚¨åˆ° FAISS...')
    faiss_chunks = [Document(page_content=chunk.page_content, metadata={**chunk.metadata, "retriever_source": "faiss"}) for chunk in chunks]
    vectorstore = FAISS.from_documents(faiss_chunks, embeddings, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)
    vectorstore.save_local(str(FAISS_INDEX))
    st.write('10. FAISS å­˜å‚¨å®Œæˆã€‚')

    # å­˜å‚¨åˆ° Elasticsearch
    st.write('11. æ­£åœ¨å°†æ–‡æœ¬å—å­˜å…¥ Elasticsearch...')
    es_chunks = [Document(page_content=chunk.page_content, metadata={**chunk.metadata, "retriever_source": "es"}) for chunk in chunks]
    try:
        ElasticsearchStore.from_documents(
            documents=es_chunks, embedding=embeddings, es_connection=es, index_name=ES_INDEX,
            strategy=ElasticsearchStore.ApproxRetrievalStrategy(hybrid=True, query_model_id="sentence-transformers/all-MiniLM-L6-v2")
        )
        st.write('12. æˆåŠŸå­˜å…¥ Elasticsearchã€‚')
    except Exception as e:
        st.error(f"å­˜å‚¨åˆ° Elasticsearch å¤±è´¥: {str(e)}")
        return []

    # éªŒè¯ Elasticsearch å­˜å‚¨
    st.write('13. éªŒè¯ Elasticsearch å­˜å‚¨...')
    try:
        test_query = {"query": {"match_all": {}}, "size": 1}
        test_result = es.search(index=ES_INDEX, body=test_query)
        if test_result['hits']['total']['value'] > 0:
            st.write("ES å­˜å‚¨éªŒè¯æˆåŠŸï¼Œé¦–æ¡è®°å½•å…ƒæ•°æ®:", test_result['hits']['hits'][0]['_source'].get('metadata'))
        else:
            st.error("ES å­˜å‚¨éªŒè¯å¤±è´¥ï¼")
    except Exception as e:
        st.error(f"éªŒè¯ Elasticsearch å­˜å‚¨å¤±è´¥: {str(e)}")

    # è·å–æ–‡ä»¶æ€»æ•°å’Œåˆ—è¡¨
    try:
        count_response = es.count(index=ES_INDEX)
        st.write(f"æ–‡ä»¶æ€»æ•°: {count_response['count']}")
    except Exception as e:
        st.error(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {"query": {"match_all": {}}, "_source": ["metadata.source"], "size": 10000}
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        file_list = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source and (filename := os.path.basename(source)) not in seen_filenames:
                seen_filenames.add(filename)
                file_type = os.path.splitext(filename)[1][1:] or 'æœªçŸ¥'
                file_size = round(os.path.getsize(source) / (1024 * 1024), 2) if os.path.exists(source) else 'æœªçŸ¥'
                file_list.append([filename, file_type, f"{file_size:.2f}"])
        return file_list
    except Exception as e:
        st.error(f"æœç´¢ Elasticsearch å¤±è´¥: {str(e)}")
        return []


def load_file_info_from_es() -> List[List[str]]:
    """ä» Elasticsearch åŠ è½½å·²å­˜å‚¨çš„æ–‡ä»¶ä¿¡æ¯"""
    es = Elasticsearch(hosts=[ES_HOST])
    if not es.indices.exists(index=ES_INDEX):
        st.write(f"ç´¢å¼• '{ES_INDEX}' ä¸å­˜åœ¨ï¼Œæœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚")
        return []

    try:
        count_response = es.count(index=ES_INDEX)
        st.write(f"æ–‡ä»¶æ€»æ•°: {count_response['count']}")
    except Exception as e:
        st.error(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {"query": {"match_all": {}}, "_source": ["metadata.source"], "size": 10000}
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        file_list = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source and (filename := os.path.basename(source)) not in seen_filenames:
                seen_filenames.add(filename)
                file_type = os.path.splitext(filename)[1][1:] or 'æœªçŸ¥'
                file_size = round(os.path.getsize(source) / (1024 * 1024), 2) if os.path.exists(source) else 'æœªçŸ¥'
                file_list.append([filename, file_type, f"{file_size:.2f}"])
        return file_list
    except Exception as e:
        st.error(f"æœç´¢ Elasticsearch å¤±è´¥: {str(e)}")
        return []


# --------------------
# RAG é—®ç­”é“¾
# --------------------

# --------------------
# RAG é—®ç­”é“¾
# --------------------
def rag_chain(question: str):
    """RAG é—®ç­”é“¾å®ç°ï¼Œåˆ†åˆ«ç”Ÿæˆ ES å’Œ FAISS çš„å›ç­”"""
    st.write('14. æ­£åœ¨åŠ è½½æ£€ç´¢å™¨...')
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_db = FAISS.load_local(str(FAISS_INDEX), embeddings, allow_dangerous_deserialization=True)
    es = Elasticsearch(hosts=[ES_HOST])

    # è‡ªå®šä¹‰ Elasticsearch æ£€ç´¢å™¨
    class CustomESRetriever(BaseRetriever, BaseModel):
        es: Elasticsearch = Field(...)
        es_index: str = Field(...)
        k: int = Field(default=15)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                body = {"query": {"match": {"text": {"query": query, "analyzer": "my_ik_analyzer"}}}, "size": self.k}
                response = self.es.search(index=self.es_index, body=body)
                return [Document(page_content=hit["_source"]["text"], metadata=hit["_source"].get("metadata", {})) for hit in response['hits']['hits']]
            except Exception as e:
                st.error(f"Elasticsearch æ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    # è‡ªå®šä¹‰ FAISS æ£€ç´¢å™¨
    class CustomFAISSRetriever(BaseRetriever, BaseModel):
        vectorstore: FAISS = Field(...)
        score_threshold: float = Field(default=0.3)  # é™ä½é˜ˆå€¼

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=20)
                # è°ƒè¯•ï¼šè¾“å‡ºæ‰€æœ‰æ–‡æ¡£çš„å¾—åˆ†
                st.write("FAISS æ£€ç´¢å¾—åˆ†ï¼š", [(doc.page_content[:50], score) for doc, score in docs_with_scores])
                filtered_docs = [doc for doc, score in docs_with_scores if score >= self.score_threshold]
                if not filtered_docs and docs_with_scores:
                    # å¦‚æœæ²¡æœ‰æ–‡æ¡£æ»¡è¶³é˜ˆå€¼ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£
                    st.write("æ²¡æœ‰æ–‡æ¡£æ»¡è¶³é˜ˆå€¼ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£...")
                    filtered_docs = [docs_with_scores[0][0]]  # è¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£
                return filtered_docs
            except Exception as e:
                st.error(f"FAISS æ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    es_retriever = CustomESRetriever(es=es, es_index=ES_INDEX)
    faiss_retriever = CustomFAISSRetriever(vectorstore=vector_db)

    # åˆ†åˆ«æ£€ç´¢ ES å’Œ FAISS
    st.write('15. æ­£åœ¨è¿›è¡Œ Elasticsearch æ£€ç´¢...')
    es_docs, es_scores = retrieve_with_scores(es_retriever, question)
    st.write(f"Elasticsearch æ£€ç´¢ç»“æœæ•°é‡: {len(es_docs)}")
    if es_docs:
        st.write(f"ç¬¬ä¸€ä¸ª ES æ–‡æ¡£: {es_docs[0]}")
    es_final_docs = [validate_metadata(Document(page_content=doc.page_content, metadata=doc.metadata)) for doc in es_docs]
    es_formatted_docs = format_documents(es_final_docs[:3])

    st.write('16. æ­£åœ¨è¿›è¡Œ FAISS æ£€ç´¢...')
    faiss_docs, faiss_scores = retrieve_with_scores(faiss_retriever, question)
    st.write(f"FAISS æ£€ç´¢ç»“æœæ•°é‡: {len(faiss_docs)}")
    if faiss_docs:
        st.write(f"ç¬¬ä¸€ä¸ª FAISS æ–‡æ¡£: {faiss_docs[0]}")
    faiss_final_docs = [validate_metadata(Document(page_content=doc.page_content, metadata=doc.metadata)) for doc in faiss_docs]
    faiss_formatted_docs = format_documents(faiss_final_docs[:3])

    # ç”Ÿæˆ ES çš„æç¤ºè¯å¹¶è°ƒç”¨æ¨¡å‹
    es_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆåŒã€æ ‡ä¹¦æ–‡æ¡£ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ï¼š
                {es_formatted_docs}
                é—®é¢˜ï¼š{question}"""
    es_answer = ""
    try:
        es_response = session.post(
            f"{OLLAMA_HOST}/api/generate", json={"model": "deepseek-r1:14b", "prompt": es_prompt, "stream": True},
            timeout=120, stream=True
        )
        for line in es_response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                es_answer += chunk
                yield "es", es_answer
    except Exception as e:
        st.error(f"Elasticsearch æ¨¡å‹ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
        yield "es", "ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    # ç”Ÿæˆ FAISS çš„æç¤ºè¯å¹¶è°ƒç”¨æ¨¡å‹
    if not faiss_formatted_docs:
        # å¦‚æœ FAISS æ£€ç´¢ç»“æœä¸ºç©ºï¼Œæä¾›é»˜è®¤å›ç­”
        yield "faiss", "FAISS æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¯èƒ½æ˜¯æŸ¥è¯¢ä¸æ–‡æ¡£å†…å®¹ä¸åŒ¹é…ã€‚è¯·å°è¯•è°ƒæ•´é—®é¢˜æˆ–ä¸Šä¼ æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚"
    else:
        faiss_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆåŒã€æ ‡ä¹¦æ–‡æ¡£ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ï¼š
                    {faiss_formatted_docs}
                    é—®é¢˜ï¼š{question}"""
        faiss_answer = ""
        try:
            faiss_response = session.post(
                f"{OLLAMA_HOST}/api/generate", json={"model": "deepseek-r1:14b", "prompt": faiss_prompt, "stream": True},
                timeout=120, stream=True
            )
            for line in faiss_response.iter_lines():
                if line:
                    chunk = json.loads(line.decode()).get("response", "")
                    faiss_answer += chunk
                    yield "faiss", faiss_answer
        except Exception as e:
            st.error(f"FAISS æ¨¡å‹ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
            yield "faiss", "ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"



# --------------------
# Streamlit UI
# --------------------
def main():
    """Streamlit ä¸»ç•Œé¢"""
    # è®¾ç½®é¡µé¢é…ç½®
    st.set_page_config(page_title="RAG (ES+FAISS+Deepseek-R1:14b)", layout="wide")

    # è‡ªå®šä¹‰ CSS æ ·å¼
    st.markdown("""
        <style>
            .stApp {
                max-width: 1400px;
                margin: 0 auto;
                background-color: #f5f5f5;
            }
            .header {
                background-color: #1e3a8a;
                color: white;
                padding: 20px;
                border-radius: 10px;
                text-align: center;
                font-size: 32px;
                font-weight: bold;
                margin-bottom: 20px;
            }
            .section-title {
                font-size: 24px;
                font-weight: bold;
                color: #1e3a8a;
                margin-top: 20px;
                margin-bottom: 10px;
            }
            .info-box {
                background-color: #e0f7fa;
                padding: 15px;
                border-radius: 8px;
                margin-bottom: 15px;
                font-size: 16px;
                color: #01579b;
            }
            .custom-button {
                background-color: #0288d1;
                color: white;
                padding: 10px 20px;
                border-radius: 5px;
                font-weight: bold;
                text-align: center;
                display: inline-block;
                cursor: pointer;
            }
            .custom-button:hover {
                background-color: #0277bd;
            }
            .file-list {
                background-color: white;
                padding: 15px;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            }
            .answer-box {
                background-color: white;
                padding: 20px;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                min-height: 200px;
                margin-top: 15px;
            }
            .stTextInput > div > div > input,
            .stTextArea > div > div > textarea {
                border: 1px solid #0288d1;
                border-radius: 5px;
                padding: 10px;
            }
        </style>
    """, unsafe_allow_html=True)

    # é¡µé¢æ ‡é¢˜
    st.markdown('<div class="header">RAG (ES+FAISS+Deepseek-R1:14b)</div>', unsafe_allow_html=True)

    # æ•´ä½“å¸ƒå±€ï¼šä¸¤åˆ—å¸ƒå±€ï¼Œå·¦ä¾§ä¸ºæ“ä½œåŒºï¼Œå³ä¾§ä¸ºå±•ç¤ºåŒº
    col1, col2 = st.columns([1, 2], gap="medium")

    # å·¦ä¾§ï¼šæ“ä½œåŒº
    with col1:
        # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
        st.markdown('<div class="section-title">ğŸ“‚ æ–‡ä»¶ä¸Šä¼ </div>', unsafe_allow_html=True)
        st.markdown('<div class="info-box">æ”¯æŒ DOCX å’Œ PDF æ–‡ä»¶ï¼Œä¸Šä¼ åå°†å­˜å‚¨åˆ° ES/FAISS åº“</div>', unsafe_allow_html=True)
        default_path = "/teamspace/studios/this_studio/elasticsearch-Faiss-Deepseek-R1-14B-RAG/AAA-database"
        file_path = st.text_input("è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„", value=default_path, key="file_path_input")
        if st.button("ä¸Šä¼ æ–‡ä»¶", key="upload_btn", help="ç‚¹å‡»ä¸Šä¼ æ–‡ä»¶åˆ° ES/FAISS"):
            if file_path:
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼Œè¯·ç¨å€™..."):
                    file_list = upload_files(file_path)
                    st.session_state['file_list'] = file_list
                    st.success("æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
            else:
                st.error("è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼")

        # æé—®éƒ¨åˆ†
        st.markdown('<div class="section-title">â“ æé—®</div>', unsafe_allow_html=True)
        st.markdown('<div class="info-box">è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç³»ç»Ÿå°†åŸºäºä¸Šä¼ çš„æ–‡ä»¶ç”Ÿæˆå›ç­”</div>', unsafe_allow_html=True)
        question = st.text_area(
            "è¾“å…¥æ‚¨çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            height=150,
            key="question_input"
        )
        if st.button("æäº¤é—®é¢˜", key="ask_btn", help="ç‚¹å‡»æäº¤é—®é¢˜"):
            if question and 'file_list' in st.session_state:
                with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”ï¼Œè¯·ç¨å€™..."):
                    # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„å®¹å™¨ç”¨äºæ˜¾ç¤º ES å’Œ FAISS çš„å›ç­”
                    es_answer_container = st.empty()
                    faiss_answer_container = st.empty()
                    counter = 0
                    for source, answer in rag_chain(question):
                        counter += 1
                        unique_key = f"answer_output_{source}_{int(time.time()*1000)}_{counter}"
                        if source == "es":
                            es_answer_container.text_area(
                                "Elasticsearch å›ç­”",
                                value=answer,
                                height=200,
                                key=unique_key
                            )
                        elif source == "faiss":
                            faiss_answer_container.text_area(
                                "FAISS å›ç­”",
                                value=answer,
                                height=200,
                                key=unique_key
                            )
            else:
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶è¾“å…¥é—®é¢˜ï¼")

    # å³ä¾§ï¼šå±•ç¤ºåŒº
    with col2:
        # å·²å­˜å‚¨æ–‡ä»¶åˆ—è¡¨
        st.markdown('<div class="section-title">ğŸ“„ å·²å­˜å‚¨æ–‡ä»¶åˆ—è¡¨</div>', unsafe_allow_html=True)
        if 'file_list' in st.session_state:
            st.dataframe(
                st.session_state['file_list'],
                column_config={"0": "æ–‡ä»¶å", "1": "æ–‡ä»¶ç±»å‹", "2": "å¤§å°(MB)"},
                use_container_width=True,
                height=200
            )
        else:
            file_list = load_file_info_from_es()
            st.session_state['file_list'] = file_list
            st.dataframe(
                file_list,
                column_config={"0": "æ–‡ä»¶å", "1": "æ–‡ä»¶ç±»å‹", "2": "å¤§å°(MB)"},
                use_container_width=True,
                height=200
            )

        # ç­”æ¡ˆå±•ç¤º
        st.markdown('<div class="section-title">ğŸ“ å›ç­”</div>', unsafe_allow_html=True)
        st.markdown('<div class="info-box">å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®</div>', unsafe_allow_html=True)
        # ä¸¤ä¸ªèŠå¤©æ¡†åˆ†åˆ«æ˜¾ç¤º ES å’Œ FAISS çš„å›ç­”
        st.markdown('<div class="answer-box" id="es-answer"></div>', unsafe_allow_html=True)
        st.markdown('<div class="answer-box" id="faiss-answer"></div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()