# -*- coding: utf-8 -*-
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import FAISS, ElasticsearchStore
from elasticsearch import Elasticsearch
from config import *  # å¯¼å…¥ OLLAMA_HOST å’Œå…¶ä»–é…ç½®
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field
from typing import List
import hashlib
import json
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import os
import time  # æ–°å¢ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€ key
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from assistant_fun import *

# é…ç½® requests é‡è¯•æœºåˆ¶
requests.adapters.DEFAULT_RETRIES = 3
session = requests.Session()
retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
session.mount('http://', HTTPAdapter(max_retries=retries))

# åŠ è½½ PDF æ–‡ä»¶ï¼ˆä¿æŒä¸å˜ï¼‰
def load_pdf(pdfpath):
    documents = []
    for filename in os.listdir(pdfpath):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(pdfpath, filename)
            loader = PyPDFLoader(pdf_path)
            document = loader.load()
            documents.extend(document)
    return documents

# ä¸Šä¼ æ–‡ä»¶å¹¶å­˜å‚¨åˆ° ES/FAISS
def upload_files(file_path):
    st.write('1. å¼€å§‹æ‰§è¡Œæ•°æ®é¢„å¤„ç†ä¸å­˜å‚¨æ­¥éª¤...')
    file_info = []

    # åˆå§‹åŒ– Elasticsearch è¿æ¥
    st.write('2. åˆå§‹åŒ– Elasticsearch å¹¶åˆ›å»ºç´¢å¼•...')
    es = Elasticsearch(hosts=[ES_HOST])
    index_settings = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "my_ik_analyzer": {
                        "type": "custom",
                        "tokenizer": "ik_max_word"
                    }
                }
            },
            # æ·»åŠ å‘é‡æ£€ç´¢æ”¯æŒï¼ˆå‡è®¾ä½¿ç”¨ dense_vector å­—æ®µï¼‰
            "number_of_shards": 1,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                "text": {
                    "type": "text",
                    "analyzer": "my_ik_analyzer",
                    "search_analyzer": "my_ik_analyzer"
                },
                "embedding": {  # æ·»åŠ å‘é‡å­—æ®µ
                    "type": "dense_vector",
                    "dims": 384  # æ ¹æ®åµŒå…¥æ¨¡å‹è°ƒæ•´ç»´åº¦ï¼Œä¾‹å¦‚ all-MiniLM-L6-v2 æ˜¯ 384
                }
            }
        }
    }
    # æ£€æŸ¥å¹¶é‡å»ºç´¢å¼•
    try:
        if es.indices.exists(index=ES_INDEX):
            st.write(f"ç´¢å¼• '{ES_INDEX}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
            es.indices.delete(index=ES_INDEX)
        es.indices.create(index=ES_INDEX, body=index_settings)
        st.write(f"æˆåŠŸåˆ›å»ºç´¢å¼• '{ES_INDEX}'")
    except Exception as e:
        st.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        return []

    st.write('3. å¤„ç†docxæ–‡ä»¶...')
    loader = DirectoryLoader(file_path, glob="**/*.docx")
    documents1 = loader.load()
    st.write(f'4. æˆåŠŸåŠ è½½ {len(documents1)} ä¸ªæ–‡æ¡£ã€‚')
    file_info1 = get_docx_info(file_path, 'docx')
    st.write('å¤„ç†pdfæ–‡ä»¶...')
    documents2 = load_pdf(file_path)
    file_info2 = get_docx_info(file_path, 'pdf')

    file_info = file_info1 + file_info2
    documents = documents1 + documents2

    st.write('5. æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†å—...')
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=["\n# ", "\n## ", "\n### ", "\n\n"])
    chunks = text_splitter.split_documents(documents)
    st.write(f'6. åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚')

    st.write('7. æ­£åœ¨åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...')
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': True})

    st.write('8. æ­£åœ¨å°†æ–‡æœ¬å—å­˜å‚¨åˆ°FAISS...')
    faiss_chunks = [Document(page_content=chunk.page_content, metadata={**chunk.metadata, "retriever_source": "faiss"}) for chunk in chunks]
    vectorstore = FAISS.from_documents(faiss_chunks, embeddings, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)
    vectorstore.save_local(str(FAISS_INDEX))
    st.write('9. FAISSå­˜å‚¨å®Œæˆã€‚')

    st.write('10. æ­£åœ¨å°†æ–‡æœ¬å—å­˜å…¥Elasticsearch...')
    es_chunks = [Document(page_content=chunk.page_content, metadata={**chunk.metadata, "retriever_source": "es"}) for chunk in chunks]
    try:
        ElasticsearchStore.from_documents(
            documents=es_chunks, embedding=embeddings, es_connection=es, index_name=ES_INDEX,
            strategy=ElasticsearchStore.ApproxRetrievalStrategy(hybrid=True, query_model_id="sentence-transformers/all-MiniLM-L6-v2")
        )
        st.write('11. æˆåŠŸå­˜å…¥Elasticsearchã€‚')
    except Exception as e:
        st.error(f"å­˜å‚¨åˆ°Elasticsearchå¤±è´¥: {str(e)}")
        return []

    st.write('12. éªŒè¯Elasticsearchå­˜å‚¨...')
    try:
        test_query = {"query": {"match_all": {}}, "size": 1}
        test_result = es.search(index=ES_INDEX, body=test_query)
        if test_result['hits']['total']['value'] > 0:
            st.write("ESå­˜å‚¨éªŒè¯æˆåŠŸï¼Œé¦–æ¡è®°å½•å…ƒæ•°æ®:", test_result['hits']['hits'][0]['_source'].get('metadata'))
        else:
            st.error("ESå­˜å‚¨éªŒè¯å¤±è´¥ï¼")
    except Exception as e:
        st.error(f"éªŒè¯Elasticsearchå­˜å‚¨å¤±è´¥: {str(e)}")

    try:
        count_response = es.count(index=ES_INDEX)
        file_count = count_response['count']
        st.write(f"æ–‡ä»¶æ€»æ•°: {file_count}")
    except Exception as e:
        st.error(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {"query": {"match_all": {}}, "_source": ["metadata.source"], "size": 10000}
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        fileList = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source:
                filename = os.path.basename(source)
                if filename in seen_filenames:
                    continue
                seen_filenames.add(filename)
                _, file_extension = os.path.splitext(filename)
                file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
                try:
                    file_size = os.path.getsize(source) / (1024 * 1024)
                    file_size = round(file_size, 2)
                except FileNotFoundError:
                    file_size = 'æœªçŸ¥'
                info = [str(filename), str(file_type), f"{file_size:.2f}"]
                fileList.append(info)
        return fileList
    except Exception as e:
        st.error(f"æœç´¢Elasticsearchå¤±è´¥: {str(e)}")
        return []

# ä» ES åŠ è½½æ–‡ä»¶ä¿¡æ¯
def load_file_info_from_es():
    es = Elasticsearch(hosts=[ES_HOST])
    try:
        if not es.indices.exists(index=ES_INDEX):
            st.write(f"ç´¢å¼• '{ES_INDEX}' ä¸å­˜åœ¨ï¼Œæœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚")
            return []
    except Exception as e:
        st.error(f"æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§å¤±è´¥: {str(e)}")
        return []

    try:
        count_response = es.count(index=ES_INDEX)
        file_count = count_response['count']
    except Exception as e:
        st.error(f"è·å–æ–‡ä»¶æ€»æ•°å¤±è´¥: {str(e)}")
        return []

    search_body = {"query": {"match_all": {}}, "_source": ["metadata.source"], "size": 10000}
    try:
        search_response = es.search(index=ES_INDEX, body=search_body)
        hits = search_response['hits']['hits']
        seen_filenames = set()
        fileList = []
        for hit in hits:
            source = hit['_source'].get('metadata', {}).get('source', '')
            if source:
                filename = os.path.basename(source)
                if filename in seen_filenames:
                    continue
                seen_filenames.add(filename)
                _, file_extension = os.path.splitext(filename)
                file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
                try:
                    file_size = os.path.getsize(source) / (1024 * 1024)
                    file_size = round(file_size, 2)
                except FileNotFoundError:
                    file_size = 'æœªçŸ¥'
                info = [str(filename), str(file_type), f"{file_size:.2f}"]
                fileList.append(info)
        return fileList
    except Exception as e:
        st.error(f"æœç´¢Elasticsearchå¤±è´¥: {str(e)}")
        return []

# RAG é—®ç­”é“¾
def rag_chain(question):
    st.write('13. æ­£åœ¨åŠ è½½æ£€ç´¢å™¨...')
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_db = FAISS.load_local(str(FAISS_INDEX), embeddings, allow_dangerous_deserialization=True)
    es = Elasticsearch(hosts=[ES_HOST])

    class CustomESRetriever(BaseRetriever, BaseModel):
        es: Elasticsearch = Field(...)
        es_index: str = Field(...)
        k: int = Field(default=15)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                body = {"query": {"match": {"text": {"query": query, "analyzer": "my_ik_analyzer"}}}, "size": self.k}
                response = self.es.search(index=self.es_index, body=body)
                return [Document(page_content=hit["_source"]["text"], metadata=hit["_source"].get("metadata", {})) for hit in response['hits']['hits']]
            except Exception as e:
                st.error(f"Elasticsearchæ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    es_retriever = CustomESRetriever(es=es, es_index=ES_INDEX)

    class CustomFAISSRetriever(BaseRetriever, BaseModel):
        vectorstore: FAISS = Field(...)
        score_threshold: float = Field(default=0.5)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            try:
                docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=20)
                return [doc for doc, score in docs_with_scores if score >= self.score_threshold]
            except Exception as e:
                st.error(f"FAISSæ£€ç´¢å¤±è´¥: {str(e)}")
                return []

    faiss_retriever = CustomFAISSRetriever(vectorstore=vector_db, score_threshold=0.5)

    st.write('å¹¶è¡Œæ£€ç´¢...')
    es_docs, es_scores = retrieve_with_scores(es_retriever, question)
    faiss_docs, faiss_scores = retrieve_with_scores(faiss_retriever, question)

    st.write('åˆå¹¶æŸ¥è¯¢ç»“æœ...')
    merged_results = merge_results(es_docs=es_docs, faiss_docs=faiss_docs, es_scores=es_scores, faiss_scores=faiss_scores, es_weight=0.4, faiss_weight=0.6, merge_strategy="reciprocal_rank")

    final_docs = [validate_metadata(Document(page_content=res.content, metadata=res.metadata)) for res in merged_results]
    formatted_docs = format_documents(final_docs[:3])
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆåŒã€æ ‡ä¹¦æ–‡æ¡£ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ï¼š
                {formatted_docs}
                é—®é¢˜ï¼š{question}"""

    try:
        response = session.post(f"{OLLAMA_HOST}/api/generate", json={"model": "deepseek-r1:14b", "prompt": prompt, "stream": True}, timeout=120, stream=True)
        full_answer = ""
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode()).get("response", "")
                full_answer += chunk
                yield full_answer
    except Exception as e:
        st.error(f"æ¨¡å‹ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
        yield "ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"

# Streamlit UI
# Streamlit UI
def main():
    st.set_page_config(page_title="RAG (ES+FAISS+Deepseek-R1:14b)", layout="wide")
    st.markdown("""
        <style>
            .stApp {max-width: 2000px;}
            .answer-box {min-height: 500px;}
            .green-button {background-color: green; color: white; padding: 10px; border-radius: 5px;}
            .blue-button {background-color: #ADD8E6; color: white; padding: 10px; border-radius: 5px;}
            .large-font {font-size: 36px;}
            .center-text {text-align: center;}
            .red-text {color: red;}
        </style>
    """, unsafe_allow_html=True)

    st.markdown('<h1 class="blue-button large-font center-text">RAG (ES+FAISS+Deepseek-R1:14b)</h1>', unsafe_allow_html=True)

    col1, col2 = st.columns([1, 3])

    with col1:
        st.markdown("## ğŸ“‚ DOCX/PDFæ–‡ä»¶è·¯å¾„")
        # è®¾ç½®é»˜è®¤è·¯å¾„ï¼Œç”¨æˆ·å¯ä¿®æ”¹
        default_path = "/teamspace/studios/this_studio/elasticsearch-Faiss-Deepseek-R1-14B-RAG/AAA-database"
        file_path = st.text_input("è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„", value=default_path, key="file_path_input")
        if st.button("æ–‡ä»¶å­˜å‚¨åˆ°ES/FAISSåº“", key="upload_btn"):
            if file_path:
                with st.spinner("å¤„ç†ä¸­..."):
                    file_list = upload_files(file_path)
                    st.session_state['file_list'] = file_list
            else:
                st.error("è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼")

        st.markdown("## â“ æé—®")
        question = st.text_area("è¾“å…¥é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ", height=100, key="question_input")
        if st.button("ğŸ” å¼€å§‹æé—®", key="ask_btn"):
            if question and 'file_list' in st.session_state:
                with st.spinner("ç”Ÿæˆå›ç­”ä¸­..."):
                    answer_container = st.empty()
                    # ä½¿ç”¨è®¡æ•°å™¨ç”Ÿæˆå”¯ä¸€ key
                    counter = 0
                    for answer in rag_chain(question):
                        counter += 1
                        unique_key = f"answer_output_{int(time.time()*1000)}_{counter}"
                        answer_container.text_area("å›ç­”", value=answer, height=400, key=unique_key)
            else:
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å¹¶è¾“å…¥é—®é¢˜ï¼")

    with col2:
        st.markdown("## å·²å­˜å‚¨æ–‡ä»¶åˆ—è¡¨")
        if 'file_list' in st.session_state:
            st.dataframe(st.session_state['file_list'], column_config={"0": "æ–‡ä»¶å", "1": "æ–‡ä»¶ç±»å‹", "2": "å¤§å°(MB)"})
        else:
            file_list = load_file_info_from_es()
            st.session_state['file_list'] = file_list
            st.dataframe(file_list, column_config={"0": "æ–‡ä»¶å", "1": "æ–‡ä»¶ç±»å‹", "2": "å¤§å°(MB)"})

        st.markdown("## ğŸ“ ç­”æ¡ˆ")
        st.markdown("*å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>*æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®", unsafe_allow_html=True)

if __name__ == "__main__":
    main()