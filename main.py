# -*- coding: utf-8 -*-
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import FAISS, ElasticsearchStore
from elasticsearch import Elasticsearch
from config import *
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field,field_validator
from typing import List, Dict, Any
import hashlib
import json
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import gradio as gr
import os
import socket
import webbrowser
from pdfminer.high_level import extract_text_to_fp
from io import StringIO
import tkinter as tk
from tkinter import filedialog
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from assistant_fun import *


requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

def load_pdf(pdfpath):
    documents=[]
    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ PDF æ–‡ä»¶
    for filename in os.listdir(pdfpath):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(pdfpath, filename)
            #print("pdf_path:",pdf_path)
            # è¯»å– PDF æ–‡ä»¶
            loader = PyPDFLoader(pdf_path)
            document = loader.load()
            #print("pdf document:", document)
            #documents.append(document)
            documents.extend(document)
    return documents

def upload_files():
    try:
       root = tk.Tk()
       # éšè—æ ¹çª—å£
       root.withdraw()
       # ç¡®ä¿çª—å£ç½®é¡¶
       root.attributes("-topmost", True)
       # æ‰“å¼€æ–‡ä»¶å¤¹é€‰æ‹©å¯¹è¯æ¡†
       file_path = filedialog.askdirectory()
       # é”€æ¯ tkinter æ ¹çª—å£
       root.destroy()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")

    """æ•°æ®é¢„å¤„ç†ä¸å­˜å‚¨"""
    print('1.å¼€å§‹æ‰§è¡Œæ•°æ®é¢„å¤„ç†ä¸å­˜å‚¨æ­¥éª¤...')
    file_info = []

    # å¤„ç†docxæ–‡ä»¶
    print('2.å¤„ç†docxæ–‡ä»¶...',file_path)
    loader = DirectoryLoader(file_path, glob="**/*.docx")
    documents1 = loader.load()
    print(f'3.æˆåŠŸåŠ è½½ {len(documents1)} ä¸ªæ–‡æ¡£ã€‚')
    file_info1 = get_docx_info(file_path,'docx')
    #print(documents1)
    print('å¤„ç†pdfæ–‡ä»¶')
    #å¤„ç†pdfæ–‡ä»¶
    documents2 = load_pdf(file_path)
    file_info2 =  get_docx_info(file_path, 'pdf')
    #print(documents2)

    #pdfï¼Œdocxåˆå¹¶ä¸€èµ·å¤„ç†
    file_info = file_info1 + file_info2
    documents = documents1+documents2

    print(file_info)
    # æ™ºèƒ½åˆ†å—
    print('4.æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†å—...')
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n# ", "\n## ", "\n### ", "\n\n"]
    )
    chunks = text_splitter.split_documents(documents)
    print(f'5.åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚')

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    print('6.æ­£åœ¨åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...')
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # å­˜å‚¨åˆ°FAISS
    print('7.æ­£åœ¨å°†æ–‡æœ¬å—å­˜å‚¨åˆ°FAISS...')
    faiss_chunks = [
            Document(
                page_content=chunk.page_content,
                metadata={
            ** chunk.metadata,
            "retriever_source": "faiss"  # æ˜ç¡®æ·»åŠ æ¥æºæ ‡è®°
            }
        ) for chunk in chunks
    ]
    vectorstore = FAISS.from_documents(faiss_chunks, embeddings,distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)
    vectorstore.save_local(str(FAISS_INDEX))
    print('8.FAISSå­˜å‚¨å®Œæˆã€‚')

    # å…ƒæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    print("æ£€æŸ¥åˆ†å—å…ƒæ•°æ®ç¤ºä¾‹ï¼š")
    if len(chunks) > 0:
        sample_chunk = chunks[0]
        print(f"å†…å®¹é•¿åº¦: {len(sample_chunk.page_content)}")
        print(f"å…ƒæ•°æ®: {sample_chunk.metadata}")

    # å­˜å‚¨åˆ°Elasticsearch
    print('9.æ­£åœ¨å°†æ–‡æœ¬å—å­˜å…¥Elasticsearch...')
    es = Elasticsearch(hosts=[ES_HOST])

    # è‡ªå®šä¹‰ç´¢å¼•è®¾ç½®ï¼Œä½¿ç”¨æ›´é€‚åˆä¸­æ–‡çš„åˆ†è¯å™¨
    index_settings = {
        "settings": {
            "analysis": {
                "analyzer": {
                    "my_ik_analyzer": {
                        "type": "custom",
                        "tokenizer": "ik_max_word"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "text": {
                    "type": "text",
                    "analyzer": "my_ik_analyzer",
                    "search_analyzer": "my_ik_analyzer"
                }
            }
        }
    }

    #åˆ é™¤ç´¢å¼•
    if es.indices.exists(index=ES_INDEX):
        es.indices.delete(index=ES_INDEX, body=index_settings)

    # ä½¿ç”¨ Elasticsearch.options() è®¾ç½®é€‰é¡¹
    es_with_options = es.options(request_timeout=10)

    # è°ƒç”¨ API æ–¹æ³•
    es_with_options.indices.create(
        index=ES_INDEX,
        body=index_settings,
        ignore=400
    )

    es_chunks = [
        Document(
            page_content=chunk.page_content,
            metadata={
                ** chunk.metadata,
                "retriever_source": "es"  # æ˜ç¡®æ·»åŠ æ¥æºæ ‡è®°
                }
            ) for chunk in chunks
    ]
    ElasticsearchStore.from_documents(
        #documents=es_chunks,
        documents=[
            Document(
                        page_content=chunk.page_content,
                        metadata={
             ** dict(chunk.metadata),  # ç¡®ä¿è½¬æ¢ä¸ºå­—å…¸
            "retriever_source": "es"
            }
            ) for chunk in es_chunks
            ],
        embedding=embeddings,
        es_connection=es,
        index_name=ES_INDEX,
        strategy=ElasticsearchStore.ApproxRetrievalStrategy(
            hybrid=True,
            query_model_id="sentence-transformers/all-MiniLM-L6-v2"  # all-mpnet-base-v2
        )
    )

    print('10.æˆåŠŸå­˜å…¥Elasticsearchã€‚')

    print('11.éªŒè¯Elasticsearchå­˜å‚¨...')
    test_query = {"query": {"match_all": {}}, "size": 1}
    test_result = es.search(index=ES_INDEX, body=test_query)
    if test_result['hits']['total']['value'] > 0:
        print("ESå­˜å‚¨éªŒè¯æˆåŠŸï¼Œé¦–æ¡è®°å½•å…ƒæ•°æ®:", test_result['hits']['hits'][0]['_source'].get('metadata'))
    else:
        print("ESå­˜å‚¨éªŒè¯å¤±è´¥ï¼")

    # æŸ¥è¯¢å·²ä¿å­˜æ–‡ä»¶çš„æ•°é‡
    count_response = es.count(index=ES_INDEX)
    file_count = count_response['count']
    print(file_count)
    # æŸ¥è¯¢æ–‡ä»¶åã€æ–‡ä»¶å¤§å°å’Œæ–‡ä»¶ç±»å‹
    search_body = {
        "query": {
            "match_all": {}
        },
        "_source": ["metadata.source"],
        "size": 10000
    }
    search_response = es.search(index=ES_INDEX, body=search_body)
    hits = search_response['hits']['hits']
    seen_filenames = set()
    fileList=[]
    for hit in hits:
        source =  hit['_source'].get('metadata', {}).get('source', '')
        #print('source',source)
        if source:
            # è·å–æ–‡ä»¶å
            filename = os.path.basename(source)
            #print('filename', filename)
            if filename in seen_filenames:
                # å¦‚æœæ–‡ä»¶åå·²ç»å‡ºç°è¿‡ï¼Œåˆ™è·³è¿‡
                continue
            # å°†æ–‡ä»¶åæ·»åŠ åˆ°å·²è§é›†åˆä¸­
            seen_filenames.add(filename)
            # è·å–æ–‡ä»¶ç±»å‹
            _, file_extension = os.path.splitext(filename)
            file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
            try:
                # è·å–æ–‡ä»¶å¤§å°ï¼ˆå‡è®¾æ–‡ä»¶åœ¨æœ¬åœ°å­˜åœ¨ï¼‰
                file_size = os.path.getsize(source) / (1024 * 1024)
                file_size = round(file_size, 2)
            except FileNotFoundError:
                file_size = 'æœªçŸ¥'
            print(f"æ–‡ä»¶å: {filename}, æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶å¤§å°: {file_size} MB")
            info = [
                str(filename),
                str(file_type),
                f"{file_size:.2f}"
            ]
            # å°†æ–‡ä»¶ä¿¡æ¯æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            fileList.append(info)
        else:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯")
    #print(fileList)
    return fileList

def Load_file_info_FrmES():
    es = Elasticsearch(hosts=[ES_HOST])
    count_response = es.count(index=ES_INDEX)
    file_count = count_response['count']

    # æŸ¥è¯¢æ–‡ä»¶åã€æ–‡ä»¶å¤§å°å’Œæ–‡ä»¶ç±»å‹
    search_body = {
        "query": {
            "match_all": {}
        },
        "_source": ["metadata.source"],
        "size": 10000
    }
    search_response = es.search(index=ES_INDEX, body=search_body)
    hits = search_response['hits']['hits']
    seen_filenames = set()
    fileList = []
    for hit in hits:
        source = hit['_source'].get('metadata', {}).get('source', '')
        if source:
            # è·å–æ–‡ä»¶å
            filename = os.path.basename(source)
            if filename in seen_filenames:
                # å¦‚æœæ–‡ä»¶åå·²ç»å‡ºç°è¿‡ï¼Œåˆ™è·³è¿‡
                continue
            # å°†æ–‡ä»¶åæ·»åŠ åˆ°å·²è§é›†åˆä¸­
            seen_filenames.add(filename)
            # è·å–æ–‡ä»¶ç±»å‹
            _, file_extension = os.path.splitext(filename)
            file_type = file_extension[1:] if file_extension else 'æœªçŸ¥'
            try:
                # è·å–æ–‡ä»¶å¤§å°ï¼ˆå‡è®¾æ–‡ä»¶åœ¨æœ¬åœ°å­˜åœ¨ï¼‰
                file_size = os.path.getsize(source) / (1024 * 1024)
                file_size = round(file_size, 2)
            except FileNotFoundError:
                file_size = 'æœªçŸ¥'
            print(f"æ–‡ä»¶å: {filename}, æ–‡ä»¶ç±»å‹: {file_type}, æ–‡ä»¶å¤§å°: {file_size} MB")
            info = [
                str(filename),
                str(file_type),
                f"{file_size:.2f}"
            ]
            # å°†æ–‡ä»¶ä¿¡æ¯æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            fileList.append(info)
        else:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯")

    return fileList

def rag_chain(question):
    """æ„å»ºRAGé“¾"""
    # åŠ è½½æ£€ç´¢å™¨
    print('13.æ­£åœ¨åŠ è½½æ£€ç´¢å™¨...')
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_db = FAISS.load_local(str(FAISS_INDEX), embeddings, allow_dangerous_deserialization=True)
    es = Elasticsearch(hosts=[ES_HOST])

    # æ„å»ºESæ£€ç´¢å™¨ï¼ˆç›´æ¥ä½¿ç”¨ElasticsearchæŸ¥è¯¢ï¼‰
    class CustomESRetriever(BaseRetriever, BaseModel):
        es: Elasticsearch = Field(...)
        es_index: str = Field(...)
        k: int = Field(default=15)

        class Config:
            arbitrary_types_allowed = True

        def _get_relevant_documents(self, query: str) -> List[Document]:
            body = {
                "query": {
                    "match": {
                        "text": {
                            "query": query,
                            "analyzer": "my_ik_analyzer"
                        }
                    }
                },
                "size": self.k
            }
            response = self.es.search(index=self.es_index, body=body)
            return [
                Document(
                    page_content=hit["_source"]["text"],
                    metadata=hit["_source"].get("metadata", {})
                ) for hit in response['hits']['hits']
            ]

    es_retriever = CustomESRetriever(
        es=es,
        es_index=ES_INDEX
    )
    class CustomFAISSRetriever(BaseRetriever, BaseModel):
        vectorstore: FAISS = Field(...)
        score_threshold: float = Field(default=0.5)

        class Config:
            arbitrary_types_allowed = True  # å…è®¸éPydanticç±»å‹

        def _get_relevant_documents(self, query: str) -> List[Document]:
            """å¸¦åˆ†æ•°è¿‡æ»¤çš„æ£€ç´¢æ–¹æ³•"""
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=20)
            return [doc for doc, score in docs_with_scores if score >= self.score_threshold]

    faiss_retriever = CustomFAISSRetriever(
        vectorstore=vector_db,  # ä½¿ç”¨å…³é”®å­—å‚æ•°
        score_threshold=0.5
    )


    #question = query["question"]
    print('å¹¶è¡Œæ£€ç´¢')
    # å¹¶è¡Œæ£€ç´¢
    #es_docs=[]
    #es_scores =[]# retrieve_with_scores(es_retriever, question)
    es_docs, es_scores = retrieve_with_scores(es_retriever, question)
    faiss_docs, faiss_scores = retrieve_with_scores(faiss_retriever, question)

    print('åˆå¹¶æŸ¥è¯¢ç»“æœ')
    #åˆå¹¶æŸ¥è¯¢ç»“æœ
    merged_results = merge_results(
        es_docs=es_docs,
        faiss_docs=faiss_docs,
        es_scores=es_scores,
        faiss_scores=faiss_scores,
        es_weight=0.4,  # å¯è°ƒæ•´æƒé‡
        faiss_weight=0.6,  # å¯è°ƒæ•´æƒé‡
        merge_strategy="reciprocal_rank"
    )
    print(merged_results)

    # æ ¼å¼è½¬æ¢ï¼ˆé€‚é…åç»­å¤„ç†ï¼‰
    final_docs = [
        validate_metadata(
            Document(
                page_content=res.content,
                metadata=res.metadata
            )
        ) for res in merged_results
    ]

    print('åˆå¹¶åç»“æœ:')
    print(final_docs)

    formatted_docs = format_documents(final_docs[:3])
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆåŒã€æ ‡ä¹¦æ–‡æ¡£ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ï¼š
                {formatted_docs}

                é—®é¢˜ï¼š{question}
                """
    # æµå¼è¯·æ±‚
    print('æ¨¡å‹å¼€å§‹é—®ç­” prompt:', prompt)
    response = session.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "deepseek-r1:14b",
            "prompt": prompt,
            "stream": True  # å¯ç”¨æµå¼
        },
        timeout=120,
        stream=True
    )

    full_answer = ""
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line.decode()).get("response", "")
            full_answer += chunk
            yield full_answer

# ä¿®æ”¹ç•Œé¢å¸ƒå±€éƒ¨åˆ†
with gr.Blocks(
        title="hi,GBEä»Šå¤©æ€ä¹ˆæ ·",
        css="""
                .gradio-container {max-width: 2000px !important}
                .answer-box {min-height: 500px !important;}
                .left-panel {padding-right: 20px; border-right: 1px solid #eee;}
                .right-panel {height: 100vh;}
                .wide-row { width: 80%; }
                .green-button {
                    background-color: green;
                    color: white; 
                }
                .blue-button {
                    background-color: #ADD8E6;
                    color: white; 
                }
               .gradio-label {
                    font-size: 8px !important;
                    font-weight: normal !important;
                }
                .gradio-container input {
                    font-size: 8px !important;
                }
                .gradio-container textbox {
                    font-size: 8px !important;
                }
                .gray-background textarea, .gray-background input[type="text"] {
                background-color: #cccccc !important;
                }      
                .large-font {
                    font-size: 72px !important;
                }     
                .bold-font {
                    font-weight: bold !important;
                }
                .center-text {
                    text-align: center !important;
                }
                .red-text {
                    color: red !important;
                }   
                """
) as demo:
    gr.Markdown("## RAG.(ES+FAISS+Deepseek-R1:14b).",  elem_classes="blue-button large-font font-weight center-text")

    with gr.Row():
        # å·¦ä¾§æ“ä½œé¢æ¿
        with gr.Column(scale=1, elem_classes="left-panel"):
            gr.Markdown("## ğŸ“‚ DOCX/PDFæ–‡ä»¶è·¯å¾„")
            with gr.Group():
                upload_btn = gr.Button("æ–‡ä»¶å­˜å‚¨åˆ°ES/FAISSåº“", variant="primary")
                upload_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)

            gr.Markdown("## â“ æé—®")
            with gr.Group():
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=4,
                    placeholder="ä¾‹å¦‚ï¼šæœ¬æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    elem_id="question-input"
                )
                ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary",elem_classes="green-button")
                #ask_btn222 = gr.Button("ğŸ” å¼€å§‹æé—®2222222", variant="primary")

        # å³ä¾§ç­”æ¡ˆæ˜¾ç¤ºåŒº
        with gr.Column(scale=3, elem_classes="right-panel"):
            gr.Markdown("## å·²ç»å­˜å‚¨æ–‡ä»¶åˆ—è¡¨ ")
            file_df = gr.Dataframe(
                headers=["æ–‡ä»¶å" , "æ–‡ä»¶ç±»å‹", "å¤§å°(MB)"],  # è¡¨æ ¼åˆ—å
                datatype=["str", "str", "str"],  # æ•°æ®ç±»å‹
                interactive=False,  # ä¸å¯äº¤äº’ç¼–è¾‘
                elem_classes="File_list-box",
                show_copy_button=True
            )

            gr.Markdown("## ğŸ“ ç­”æ¡ˆ",elem_classes="blue-button")
            answer_output = gr.Textbox(
                label="å›ç­”",
                interactive=False,
                lines=25,
                elem_classes="answer-box",
                autoscroll=True,
                show_copy_button=True
            )
            gr.Markdown("""
            <div class="footer-note">
                *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    # åœ¨ç•Œé¢ç»„ä»¶å®šä¹‰ä¹‹åæ·»åŠ æŒ‰é’®äº‹ä»¶
    ask_btn.click(
        fn=rag_chain,
        inputs=question_input,
        outputs=[answer_output],
        show_progress="hidden"
    )

    upload_btn.click(
        fn=upload_files,
        inputs=None,
        outputs=file_df
    )

    # åœ¨é¡µé¢åŠ è½½æ—¶æ˜¾ç¤º ES æ–‡ä»¶åˆ—è¡¨
    demo.load(
        fn=lambda: Load_file_info_FrmES(),
        inputs=None,
        outputs=file_df
    )
# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†ä¸ºå…¼å®¹å†™æ³•
demo._js = """
function gradioApp() {
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:14b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False

        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)

    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)

    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)

        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")